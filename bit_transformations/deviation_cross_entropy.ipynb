{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aafe5546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kan\n",
    "import kan.utils as ku\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import qutip as qt\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from libraries import utils\n",
    "from libraries import magnetization\n",
    "from libraries import j1j2_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3929dc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.495\n",
      "1365\n",
      "torch.Size([924, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taoha\\AppData\\Local\\Temp\\ipykernel_38192\\2901660590.py:20: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  x = (-1) ** np.array(x)\n"
     ]
    }
   ],
   "source": [
    "N=12; J1=1\n",
    "J2s = np.linspace(0, 0.5 - 0.5/100, 100)\n",
    "print(J2s[-1])\n",
    "gss = [qt.qload(f'../J1J2_info/N{N}/n{N}gs_J2_{round(J2, 3)}') for J2 in J2s]\n",
    "states = utils.get_nonzero_states(N, gss[0], 1e-10)[0]\n",
    "signs = [utils.get_nonzero_states(N, gs, 1e-10)[1] for gs in gss]\n",
    "\n",
    "nd = 0b010101010101 # this never deviates, let this be of negative sign cos(pi/4 * amplitude)\n",
    "index = states.index(nd)\n",
    "print(nd)\n",
    "for i in range(len(signs)):\n",
    "    if signs[i][index] > 0:\n",
    "        signs[i] = -1 * np.array(signs[i])\n",
    "    else:\n",
    "        signs[i] = np.array(signs[i])\n",
    "\n",
    "devs = [(signs[0] != signs[i]).astype(float) for i in range(len(signs))]\n",
    "\n",
    "def rfft(x):\n",
    "    x = (-1) ** np.array(x)\n",
    "    return np.fft.rfft(x)[1:]\n",
    "\n",
    "input = utils.generate_input_samples(N, states)\n",
    "print(input.shape)\n",
    "ft_input = []\n",
    "for x in input:\n",
    "    amp_phase = []\n",
    "    ft = rfft(x)\n",
    "    for c in ft:\n",
    "        amp_phase.append(np.abs(c))\n",
    "        amp_phase.append(np.angle(c) % (2 * np.pi))\n",
    "    ft_input.append(amp_phase)\n",
    "ft_input = torch.tensor(ft_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e68bf194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0 0.0\n",
      "1 0.005 0.0\n",
      "2 0.01 0.0\n",
      "3 0.015 0.0\n",
      "4 0.02 0.0\n",
      "5 0.025 0.0\n",
      "6 0.03 0.0\n",
      "7 0.035 0.0\n",
      "8 0.04 0.0\n",
      "9 0.045 0.0\n",
      "10 0.05 0.0\n",
      "11 0.055 0.0\n",
      "12 0.06 0.0\n",
      "13 0.065 0.0\n",
      "14 0.07 0.0\n",
      "15 0.075 0.0\n",
      "16 0.08 0.0\n",
      "17 0.085 0.0\n",
      "18 0.09 0.0\n",
      "19 0.095 0.0\n",
      "20 0.1 0.0\n",
      "21 0.105 0.0\n",
      "22 0.11 0.0\n",
      "23 0.115 0.0\n",
      "24 0.12 0.0\n",
      "25 0.125 12.0\n",
      "26 0.13 12.0\n",
      "27 0.135 12.0\n",
      "28 0.14 12.0\n",
      "29 0.145 12.0\n",
      "30 0.15 12.0\n",
      "31 0.155 12.0\n",
      "32 0.16 12.0\n",
      "33 0.165 12.0\n",
      "34 0.17 36.0\n",
      "35 0.175 36.0\n",
      "36 0.18 36.0\n",
      "37 0.185 84.0\n",
      "38 0.19 108.0\n",
      "39 0.195 108.0\n",
      "40 0.2 108.0\n",
      "41 0.205 108.0\n",
      "42 0.21 96.0\n",
      "43 0.215 96.0\n",
      "44 0.22 96.0\n",
      "45 0.225 96.0\n",
      "46 0.23 96.0\n",
      "47 0.235 96.0\n",
      "48 0.24 120.0\n",
      "49 0.245 120.0\n",
      "50 0.25 120.0\n",
      "51 0.255 120.0\n",
      "52 0.26 144.0\n",
      "53 0.265 144.0\n",
      "54 0.27 144.0\n",
      "55 0.275 144.0\n",
      "56 0.28 144.0\n",
      "57 0.285 144.0\n",
      "58 0.29 192.0\n",
      "59 0.295 192.0\n",
      "60 0.3 192.0\n",
      "61 0.305 192.0\n",
      "62 0.31 240.0\n",
      "63 0.315 240.0\n",
      "64 0.32 264.0\n",
      "65 0.325 264.0\n",
      "66 0.33 264.0\n",
      "67 0.335 264.0\n",
      "68 0.34 240.0\n",
      "69 0.345 264.0\n",
      "70 0.35 264.0\n",
      "71 0.355 312.0\n",
      "72 0.36 336.0\n",
      "73 0.365 336.0\n",
      "74 0.37 336.0\n",
      "75 0.375 336.0\n",
      "76 0.38 336.0\n",
      "77 0.385 336.0\n",
      "78 0.39 288.0\n",
      "79 0.395 288.0\n",
      "80 0.4 312.0\n",
      "81 0.405 312.0\n",
      "82 0.41 288.0\n",
      "83 0.415 288.0\n",
      "84 0.42 300.0\n",
      "85 0.425 300.0\n",
      "86 0.43 300.0\n",
      "87 0.435 300.0\n",
      "88 0.44 300.0\n",
      "89 0.445 300.0\n",
      "90 0.45 300.0\n",
      "91 0.455 300.0\n",
      "92 0.46 300.0\n",
      "93 0.465 300.0\n",
      "94 0.47 300.0\n",
      "95 0.475 300.0\n",
      "96 0.48 300.0\n",
      "97 0.485 300.0\n",
      "98 0.49 300.0\n",
      "99 0.495 300.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(devs)):\n",
    "    print(i, round(J2s[i], 3), sum(devs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bb7a04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(12.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(devs[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad03db06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6927897334098816\n",
      "0.010283734649419785\n",
      "0.0017282887129113078\n",
      "0.0005366969853639603\n",
      "0.000205744756385684\n",
      "5.128790144226514e-05\n",
      "tensor(0., grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2)\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(N, 1),\n",
    "    nn.Sigmoid(),\n",
    "    #nn.Linear(3, 1),\n",
    "    #nn.Sigmoid()\n",
    ")\n",
    "index = 70\n",
    "opt = torch.optim.Adam(mlp.parameters(), lr=0.1)\n",
    "floss = torch.nn.BCELoss()\n",
    "epochs = 50000; datarate = epochs // 5\n",
    "for i in range(epochs):\n",
    "    def closure():\n",
    "        opt.zero_grad()\n",
    "        loss = floss(mlp(ft_input), torch.tensor(devs[index], dtype=torch.float32).reshape(-1, 1))\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    loss = opt.step(closure)\n",
    "    if i % datarate == 0:\n",
    "        print(loss.item())\n",
    "print(loss.item())\n",
    "print(floss(torch.round(mlp(ft_input)), torch.tensor(devs[index], dtype=torch.float32).reshape(-1, 1)))\n",
    "# seems 1 hidden layer MLP can work for anything with very good loss (there may be too many parameters)\n",
    "# up to index 71 dev string can be done with no hidden layers (linear separator) seems mostly just like based on lowest freq\n",
    "# more requires more training epochs\n",
    "# it seems like index >= 72 becomes more difficult??\n",
    "# seems >=72 can easily be done with non-linear separator\n",
    "# (N, 3, 1) is minimal for index 92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e8e9ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 7.7016e+01, -1.1658e-02,  8.5867e+01,  1.5922e-02, -1.2201e+01,\n",
       "          -8.4497e-02,  2.0635e+00, -8.0911e-02, -3.5666e+00, -2.8490e-02,\n",
       "          -3.3578e+01, -1.0719e-02]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-561.0061], requires_grad=True)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n for n in mlp.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69598154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a19e9899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "1.076793909072876\n",
      "2.2413047418012866e-08\n",
      "1.8615663677223893e-08\n",
      "1.8615663677223893e-08\n",
      "1.8615663677223893e-08\n",
      "1.8615663677223893e-08\n",
      "tensor(0., grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2)\n",
    "kan_model = kan.KAN(width=[N, 1])\n",
    "kan_model(ft_input);\n",
    "for i in range(0, N, 2):\n",
    "    kan_model.fix_symbolic(0, i, 0, 'x', verbose=False, log_history=False)\n",
    "    kan_model.fix_symbolic(0, i + 1, 0, '0', verbose=False, log_history=False)\n",
    "index = 70\n",
    "opt = torch.optim.LBFGS(kan_model.parameters(), lr=0.1)\n",
    "floss = torch.nn.BCELoss()\n",
    "epochs = 100; datarate = epochs // 5\n",
    "for i in range(epochs):\n",
    "    def closure():\n",
    "        opt.zero_grad()\n",
    "        loss = floss(torch.sigmoid(kan_model(ft_input)), torch.tensor(devs[index], dtype=torch.float32).reshape(-1, 1))\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    loss = opt.step(closure)\n",
    "    if i % datarate == 0:\n",
    "        print(loss.item())\n",
    "print(loss.item())\n",
    "print(floss(torch.round(torch.sigmoid(kan_model(ft_input))), torch.tensor(devs[index], dtype=torch.float32).reshape(-1, 1)))\n",
    "# okay this basically has same behavior as kan sort of makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd57cd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(floss(torch.round(torch.sigmoid(kan_model(ft_input))), torch.tensor(devs[index], dtype=torch.float32).reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe424224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x200 with 14 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kan_model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d770cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 148.634 x_{1} - 41.268 x_{11} + 150.147 x_{3} + 12.925 x_{5} + 17.412 x_{7} + 18.797 x_{9} - 1239.845$"
      ],
      "text/plain": [
       "148.634*x_1 - 41.268*x_11 + 150.147*x_3 + 12.925*x_5 + 17.412*x_7 + 18.797*x_9 - 1239.845"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ku.ex_round(kan_model.symbolic_formula()[0][0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce97299f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([924, 24])\n",
      "1.8167599439620972\n",
      "6.525795726020078e-08\n",
      "6.525795726020078e-08\n",
      "6.525795726020078e-08\n",
      "6.525795726020078e-08\n",
      "6.525795726020078e-08\n",
      "tensor(0., grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' params from cubic amp training\\n[Parameter containing:\\n tensor([[1196.2830, -804.1158, -223.7516, -223.8448, -398.4391,    5.0210,\\n          -213.9757,  230.4781,   44.6948,  129.5988,  137.5468,   12.5491,\\n             9.3357,  -19.6510,   -3.7282,  -17.5709,  -18.8858,   -5.8622]],\\n        requires_grad=True),\\n Parameter containing:\\n tensor([4.8512], requires_grad=True)]\\n '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we try taylor expansion include x^2 term here\n",
    "quad_ft = torch.hstack((ft_input, ft_input ** 2))#, ft_input ** 3))\n",
    "print(quad_ft.shape)\n",
    "\n",
    "torch.manual_seed(2)\n",
    "man_tensor = torch.tensor([[1196.2830, 0, -804.1158, 0, -223.7516, 0, -223.8448, 0, -398.4391, 0, 5.0210, 0,\n",
    "          -213.9757, 0, 230.4781, 0,  44.6948, 0, 129.5988, 0, 137.5468, 0,  12.5491, 0,]], requires_grad=True)\n",
    "          #   9.3357, 0, -19.6510, 0, -3.7282, 0, -17.5709, 0, -18.8858, 0, -5.8622, 0]],\n",
    "        #requires_grad=True)\n",
    "man_bias = torch.tensor([4.8512], requires_grad=True)\n",
    "man_linear = nn.Linear(2*N, 1)\n",
    "# man_linear.weight = nn.Parameter(man_tensor)\n",
    "# man_linear.bias = nn.Parameter(man_bias)\n",
    "quad_mlp = nn.Sequential(\n",
    "    man_linear,\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "index = 72\n",
    "opt = torch.optim.LBFGS(quad_mlp.parameters(), lr=0.1)\n",
    "floss = torch.nn.BCELoss()\n",
    "epochs = 50000; datarate = epochs // 5\n",
    "for i in range(epochs):\n",
    "    def closure():\n",
    "        opt.zero_grad()\n",
    "        loss = floss(quad_mlp(quad_ft), torch.tensor(devs[index], dtype=torch.float32).reshape(-1, 1))\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    loss = opt.step(closure)\n",
    "    if i % datarate == 0:\n",
    "        print(loss.item())\n",
    "print(loss.item())\n",
    "print(floss(torch.round(quad_mlp(quad_ft)), torch.tensor(devs[index], dtype=torch.float32).reshape(-1, 1)))\n",
    "# this seems to work until index 83 (with manual seed 2)\n",
    "# with >=84 it seems to get stuck at 0.258 loss, tried seed up to 5 \n",
    "# simply trying cubic parameters without the cubic parts also doesn't work \n",
    "# even though the cubic factors are sort of small, assuming that the inputs are large since cubes\n",
    "\"\"\" params from cubic amp training\n",
    "[Parameter containing:\n",
    " tensor([[1196.2830, -804.1158, -223.7516, -223.8448, -398.4391,    5.0210,\n",
    "          -213.9757,  230.4781,   44.6948,  129.5988,  137.5468,   12.5491,\n",
    "             9.3357,  -19.6510,   -3.7282,  -17.5709,  -18.8858,   -5.8622]],\n",
    "        requires_grad=True),\n",
    " Parameter containing:\n",
    " tensor([4.8512], requires_grad=True)]\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ea44f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 1.4656e+02,  1.2702e+00,  6.8296e+01,  1.1211e+00,  1.2955e+01,\n",
       "           3.9920e-01,  2.7553e+01,  4.0684e+00,  6.0542e+00,  2.3855e+00,\n",
       "           2.7811e+01, -1.6409e-02, -2.1145e+01, -1.4093e-01, -1.2998e+01,\n",
       "          -1.3497e-01, -8.4991e+00, -5.9915e-02, -1.1685e+01, -6.5229e-01,\n",
       "          -8.9387e+00, -3.9143e-01, -1.2097e+01,  5.0671e-02]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.9789], requires_grad=True)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n for n in quad_mlp.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0670338a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1520, -0.0277, -0.3233]], requires_grad=True) Parameter containing:\n",
      "tensor([-0.3272], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[1., 2., 3.]], requires_grad=True) Parameter containing:\n",
      "tensor([-1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = nn.Linear(3, 1)\n",
    "print(x.weight, x.bias)\n",
    "x.weight = nn.Parameter(torch.tensor([[1.0, 2.0, 3.0]]))\n",
    "x.bias = nn.Parameter(torch.tensor([-1.0]))\n",
    "print(x.weight, x.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "393da3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 1.4656e+02,  1.2702e+00,  6.8296e+01,  1.1211e+00,  1.2955e+01,\n",
       "           3.9920e-01,  2.7553e+01,  4.0684e+00,  6.0542e+00,  2.3855e+00,\n",
       "           2.7811e+01, -1.6409e-02, -2.1145e+01, -1.4093e-01, -1.2998e+01,\n",
       "          -1.3497e-01, -8.4991e+00, -5.9915e-02, -1.1685e+01, -6.5229e-01,\n",
       "          -8.9387e+00, -3.9143e-01, -1.2097e+01,  5.0671e-02]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.9789], requires_grad=True)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n for n in quad_mlp.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfb9bc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([924, 36])\n",
      "7.259513854980469\n",
      "1.74056165036518e-08\n",
      "1.74056165036518e-08\n",
      "1.74056165036518e-08\n",
      "1.74056165036518e-08\n",
      "1.74056165036518e-08\n",
      "tensor(0., grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# we try taylor expansion include x^3 term here\n",
    "cub_ft = torch.hstack((ft_input, ft_input ** 2, ft_input ** 3))\n",
    "print(cub_ft.shape)\n",
    "\n",
    "# it seems manual seed 2 is much better than man seed 1??\n",
    "torch.manual_seed(2)\n",
    "cub_mlp = nn.Sequential(\n",
    "    nn.Linear(3*N, 1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "index = 84\n",
    "opt = torch.optim.LBFGS(cub_mlp.parameters(), lr=0.1)\n",
    "floss = torch.nn.BCELoss()\n",
    "epochs = 50000; datarate = epochs // 5\n",
    "for i in range(epochs):\n",
    "    def closure():\n",
    "        opt.zero_grad()\n",
    "        loss = floss(cub_mlp(cub_ft), torch.tensor(devs[index], dtype=torch.float32).reshape(-1, 1))\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    loss = opt.step(closure)\n",
    "    if i % datarate == 0:\n",
    "        print(loss.item())\n",
    "print(loss.item())\n",
    "print(floss(torch.round(cub_mlp(cub_ft)), torch.tensor(devs[index], dtype=torch.float32).reshape(-1, 1)))\n",
    "# for some reason it seems like this struggles with man seed 1 but not 2??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "398bccaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 5.5488e+03,  1.3914e+02, -3.8474e+03, -1.3676e-01, -1.1280e+03,\n",
       "           6.9028e+01, -9.3511e+02, -4.2222e+01, -1.8845e+03,  1.0042e+02,\n",
       "          -5.2761e+00,  7.5677e-03, -9.6418e+02, -5.3038e+01,  1.0876e+03,\n",
       "          -4.5746e+00,  2.0668e+02, -2.0874e+01,  5.7649e+02,  2.0002e+01,\n",
       "           6.1209e+02, -3.7364e+01, -1.3282e+01,  9.4748e-02,  3.9969e+01,\n",
       "           5.4173e+00, -9.1240e+01,  7.7370e-01, -1.4579e+01,  1.9856e+00,\n",
       "          -7.9998e+01, -2.4192e+00, -8.3323e+01,  3.7031e+00, -6.4266e+00,\n",
       "          -1.5480e-01]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([21.0776], requires_grad=True)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n for n in cub_mlp.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fd036b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.7274e+00, 4.5776e-16, 2.8284e+00, 1.1102e-16, 2.0706e+00, 0.0000e+00],\n",
       "        [6.6921e+00, 2.0000e+00, 0.0000e+00, 3.4641e+00, 1.7932e+00, 4.0000e+00],\n",
       "        [5.8186e+00, 3.4641e+00, 2.8284e+00, 3.4641e+00, 2.4786e+00, 0.0000e+00],\n",
       "        ...,\n",
       "        [5.8186e+00, 3.4641e+00, 2.8284e+00, 3.4641e+00, 2.4786e+00, 0.0000e+00],\n",
       "        [6.6921e+00, 2.0000e+00, 0.0000e+00, 3.4641e+00, 1.7932e+00, 4.0000e+00],\n",
       "        [7.7274e+00, 4.5776e-16, 2.8284e+00, 1.1102e-16, 2.0706e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_input[:, ::2] # amplitudes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5205ad94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([924, 18])\n",
      "8.566025733947754\n",
      "1.4223613398200996e-08\n",
      "1.4223613398200996e-08\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[1;32m---> 22\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m datarate \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\taoha\\anaconda3\\envs\\kans\\Lib\\site-packages\\torch\\optim\\optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m             )\n\u001b[1;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\taoha\\anaconda3\\envs\\kans\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\taoha\\anaconda3\\envs\\kans\\Lib\\site-packages\\torch\\optim\\lbfgs.py:393\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    391\u001b[0m q \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mneg()\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_old \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 393\u001b[0m     al[i] \u001b[38;5;241m=\u001b[39m \u001b[43mold_stps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m ro[i]\n\u001b[0;32m    394\u001b[0m     q\u001b[38;5;241m.\u001b[39madd_(old_dirs[i], alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mal[i])\n\u001b[0;32m    396\u001b[0m \u001b[38;5;66;03m# multiply by initial Hessian\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# r/d is the final direction\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# we try taylor expansion include x^3 term here with only amplitudes\n",
    "amp_input = ft_input[:, ::2]\n",
    "\n",
    "cub_amp = torch.hstack((amp_input, amp_input ** 2, amp_input ** 3))\n",
    "print(cub_amp.shape)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "cub_amp_mlp= nn.Sequential(\n",
    "    nn.Linear(3*N // 2, 1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "index = 84\n",
    "opt = torch.optim.LBFGS(cub_amp_mlp.parameters(), lr=0.1)\n",
    "floss = torch.nn.BCELoss()\n",
    "epochs = 50000; datarate = epochs // 5\n",
    "for i in range(epochs):\n",
    "    def closure():\n",
    "        opt.zero_grad()\n",
    "        loss = floss(cub_amp_mlp(cub_amp), torch.tensor(devs[index], dtype=torch.float32).reshape(-1, 1))\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    loss = opt.step(closure)\n",
    "    if i % datarate == 0:\n",
    "        print(loss.item())\n",
    "print(loss.item())\n",
    "print(floss(torch.round(cub_amp_mlp(cub_amp)), torch.tensor(devs[index], dtype=torch.float32).reshape(-1, 1)))\n",
    "# basically we gotta play around with man seed (1 default)\n",
    "# fails ind=72, passes 80, 83, 84, seems like most else\n",
    "# man seed 2 works for ind=72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1230b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[1196.2830, -804.1158, -223.7516, -223.8448, -398.4391,    5.0210,\n",
       "          -213.9757,  230.4781,   44.6948,  129.5988,  137.5468,   12.5491,\n",
       "             9.3357,  -19.6510,   -3.7282,  -17.5709,  -18.8858,   -5.8622]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([4.8512], requires_grad=True)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n for n in cub_amp_mlp.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1324f98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([924, 18])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cub_amp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7784935d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got int"
     ]
    }
   ],
   "source": [
    "torch.vstack((1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c01b963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([92400, 18]) torch.Size([92400, 1]) torch.Size([92400, 1])\n",
      "torch.float32 torch.float32 torch.float32\n"
     ]
    }
   ],
   "source": [
    "# we try taylor expansion include x^3 term here with only amplitudes and also J2 term\n",
    "\n",
    "cub_amp_j2 = torch.vstack((cub_amp,) * len(J2s))\n",
    "j2_data = torch.vstack(tuple(torch.tensor([j2] * cub_amp.shape[0], dtype=torch.float32).reshape(-1, 1) for j2 in J2s))\n",
    "j2_signs = torch.vstack(tuple(torch.tensor(devs[i], dtype=torch.float32).reshape(-1, 1) for i in range(len(devs))))\n",
    "print(cub_amp_j2.shape, j2_data.shape, j2_signs.shape)\n",
    "print(cub_amp_j2.dtype, j2_data.dtype, j2_signs.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cf5660c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.055763244628906\n",
      "84.16883087158203\n",
      "84.16883087158203\n",
      "84.16883087158203\n",
      "84.16883087158203\n",
      "84.16883087158203\n",
      "tensor(84.1688, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# we try taylor expansion include x^3 term here with only amplitudes and also J2 term\n",
    "\n",
    "torch.manual_seed(5)\n",
    "amp_cub = nn.Sequential(\n",
    "    nn.Linear(3*N // 2, 1),\n",
    ")\n",
    "\n",
    "j2_lin = nn.Sequential(\n",
    "    nn.Linear(1, 1),\n",
    ")\n",
    "\n",
    "opt = torch.optim.LBFGS(itertools.chain(amp_cub.parameters(), j2_lin.parameters()), lr=0.1)\n",
    "floss = torch.nn.BCELoss()\n",
    "epochs = 1000; datarate = epochs // 5\n",
    "for i in range(epochs):\n",
    "    def closure():\n",
    "        opt.zero_grad()\n",
    "        pred = torch.sigmoid(amp_cub(cub_amp_j2) * j2_lin(j2_data))\n",
    "        loss = floss(pred, j2_signs)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    loss = opt.step(closure)\n",
    "    if i % datarate == 0:\n",
    "        print(loss.item())\n",
    "print(loss.item())\n",
    "print(floss(torch.round(torch.sigmoid(amp_cub(cub_amp_j2) * j2_lin(j2_data))), j2_signs))\n",
    "# 0.2232, 0.2527, 0.2276, 0.2274, 0.2232, 84.169 local min for seeds 0-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1d209a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.421175479888916\n",
      "0.19052742421627045\n",
      "0.19052742421627045\n",
      "0.19052742421627045\n",
      "0.19052742421627045\n",
      "0.19052742421627045\n",
      "tensor(15.8312, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# try including J2^2 \n",
    "\n",
    "j2_data_quad = torch.hstack((j2_data, j2_data ** 2))\n",
    "\n",
    "torch.manual_seed(5)\n",
    "amp_cub = nn.Sequential(\n",
    "    nn.Linear(3*N // 2, 1),\n",
    ")\n",
    "\n",
    "j2_quad = nn.Sequential(\n",
    "    nn.Linear(2, 1),\n",
    ")\n",
    "\n",
    "opt = torch.optim.LBFGS(itertools.chain(amp_cub.parameters(), j2_quad.parameters()), lr=0.1)\n",
    "floss = torch.nn.BCELoss()\n",
    "epochs = 1000; datarate = epochs // 5\n",
    "for i in range(epochs):\n",
    "    def closure():\n",
    "        opt.zero_grad()\n",
    "        pred = torch.sigmoid(amp_cub(cub_amp_j2) * j2_quad(j2_data_quad))\n",
    "        loss = floss(pred, j2_signs)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    loss = opt.step(closure)\n",
    "    if i % datarate == 0:\n",
    "        print(loss.item())\n",
    "print(loss.item())\n",
    "print(floss(torch.round(torch.sigmoid(amp_cub(cub_amp_j2) * j2_quad(j2_data_quad))), j2_signs))\n",
    "# most losses around 0.1903 and 0.1905 for various seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a63ef32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.303403854370117\n",
      "0.21287591755390167\n",
      "0.21181952953338623\n",
      "0.21181946992874146\n",
      "0.21181945502758026\n",
      "0.21181945502758026\n",
      "tensor(0.1583, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# try including J2^3\n",
    "\n",
    "j2_data_cub = torch.hstack((j2_data, j2_data ** 2, j2_data ** 3))\n",
    "\n",
    "torch.manual_seed(2)\n",
    "amp_cub = nn.Sequential(\n",
    "    nn.Linear(3*N // 2, 1),\n",
    ")\n",
    "\n",
    "j2_cub = nn.Sequential(\n",
    "    nn.Linear(3, 1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "opt = torch.optim.LBFGS(itertools.chain(amp_cub.parameters(), j2_cub.parameters()), lr=0.05)\n",
    "floss = torch.nn.BCELoss()\n",
    "epochs = 200; datarate = epochs // 5\n",
    "for i in range(epochs):\n",
    "    def closure():\n",
    "        opt.zero_grad()\n",
    "        pred = torch.sigmoid(amp_cub(cub_amp_j2) * j2_cub(j2_data_cub))\n",
    "        loss = floss(pred, j2_signs)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    loss = opt.step(closure)\n",
    "    if i % datarate == 0:\n",
    "        print(loss.item())\n",
    "print(loss.item())\n",
    "print(torch.nn.MSELoss()(torch.round(torch.sigmoid(amp_cub(cub_amp_j2) * j2_cub(j2_data_cub))), j2_signs))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bdb9a039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([92400, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j2_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6951eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.443916320800781\n",
      "0.21227507293224335\n",
      "0.21161410212516785\n",
      "0.6931473612785339\n",
      "0.6931473612785339\n",
      "0.6931473612785339\n",
      "tensor(0.1782, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# try form f(x) * (a*J2 + b*J2^3 + c*J2^3 + d)\n",
    "\n",
    "j2_data_cub = torch.hstack((torch.ones(j2_data.shape), j2_data, j2_data ** 2, j2_data ** 3))\n",
    "\n",
    "torch.manual_seed(0)\n",
    "amp_cub = nn.Sequential(\n",
    "    nn.Linear(3*N // 2, 1),\n",
    ")\n",
    "\n",
    "j2_cub = nn.Sequential(\n",
    "    nn.Linear(4, 1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "opt = torch.optim.LBFGS(itertools.chain(amp_cub.parameters(), j2_cub.parameters()), lr=0.05)\n",
    "floss = torch.nn.BCELoss()\n",
    "epochs = 200; datarate = epochs // 5\n",
    "for i in range(epochs):\n",
    "    def closure():\n",
    "        opt.zero_grad()\n",
    "        pred = torch.sigmoid(amp_cub(cub_amp_j2) * j2_cub(j2_data_cub))\n",
    "        loss = floss(pred, j2_signs)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    loss = opt.step(closure)\n",
    "    if i % datarate == 0:\n",
    "        print(loss.item())\n",
    "print(loss.item())\n",
    "print(torch.nn.MSELoss()(torch.round(torch.sigmoid(amp_cub(cub_amp_j2) * j2_cub(j2_data_cub))), j2_signs))\n",
    "# seeds 0-2 go to 0.693, 0.209, 15., "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eda5472a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([924, 18])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cub_amp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3dc5767b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 1.0353, 1.4641, 1.7931, 2.0000, 2.0705, 2.4786, 2.8284, 3.0119,\n",
       "        3.4641, 3.8637, 4.0000, 4.4721, 4.7883, 4.8990, 5.4641, 5.6568, 5.8186,\n",
       "        6.6921, 7.7274])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cub_amp[:, 0].round(decimals=5).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "32fcaa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307200\n"
     ]
    }
   ],
   "source": [
    "prod = 1\n",
    "for i in range(6):\n",
    "    prod *= len(cub_amp[:, i].round(decimals=5).unique())\n",
    "print(prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c090b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([92400, 18])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cub_amp_j2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64e3eb1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        ...,\n",
       "        [0.4950],\n",
       "        [0.4950],\n",
       "        [0.4950]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j2_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b326f56c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7a109996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.007755279541016\n",
      "0.1328033059835434\n",
      "0.07802457362413406\n",
      "0.0769345834851265\n",
      "0.0769185945391655\n",
      "0.07690788060426712\n",
      "tensor(0.0525, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# try form f(x) + g(x) * J2?? where f, g are functions of up to amp^3\n",
    "\n",
    "torch.manual_seed(3)\n",
    "f = nn.Sequential(nn.Linear(3 * N // 2, 1))\n",
    "g = nn.Sequential(nn.Linear(3 * N // 2, 1))\n",
    "\n",
    "opt = torch.optim.LBFGS(itertools.chain(f.parameters(), g.parameters()), lr=0.05)\n",
    "floss = torch.nn.BCELoss()\n",
    "epochs = 200; datarate = epochs // 5\n",
    "for i in range(epochs):\n",
    "    def closure():\n",
    "        opt.zero_grad()\n",
    "        pred = torch.sigmoid(f(cub_amp_j2) + g(cub_amp_j2) * j2_data)\n",
    "        loss = floss(pred, j2_signs)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    loss = opt.step(closure)\n",
    "    if i % datarate == 0:\n",
    "        print(loss.item())\n",
    "print(loss.item())\n",
    "print(torch.nn.MSELoss()(torch.round(torch.sigmoid(f(cub_amp_j2) + g(cub_amp_j2) * j2_data)), j2_signs))\n",
    "# MAN SEED ONE LOOKS REALLY GOOD 0.077 loss, 0.0530 after rounding\n",
    "# seed 3 also works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "47affc06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4896.000002324581"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.05298701301217079 * 92400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3369ee39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05298701301217079\n"
     ]
    }
   ],
   "source": [
    "print(torch.nn.MSELoss()(torch.round(torch.sigmoid(f(cub_amp_j2) + g(cub_amp_j2) * j2_data)), j2_signs).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "127a6f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 9.4554e+02,  1.2648e+02, -6.5107e+01, -1.0524e+02, -4.0350e+02,\n",
       "           1.2067e+00, -1.8690e+02, -2.6945e+01, -1.2389e+01, -9.2918e+00,\n",
       "           1.9372e+02,  2.5092e+00,  9.6785e+00, -2.3714e+00, -2.0847e+00,\n",
       "          -9.3538e-02, -3.1342e+01, -4.1929e+00]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.3658], requires_grad=True)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n for n in f.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f25b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 4.3857e+02, -7.4293e+01, -1.1010e+01, -2.0925e+00, -2.5207e+02,\n",
       "           4.3039e-01, -9.7949e+01, -8.5202e+01,  4.1604e+01,  9.5317e+01,\n",
       "          -9.0487e+01,  1.3474e+00,  6.1349e+00,  1.7650e+01, -1.1079e+00,\n",
       "          -1.4105e+01,  3.0260e+01, -6.4721e-01]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-1.9672], requires_grad=True)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n for n in g.parameters()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfc0d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.92538833618164\n",
      "0.06094572693109512\n",
      "0.02474370039999485\n",
      "0.008290494792163372\n",
      "0.007655133493244648\n",
      "0.007642410695552826\n",
      "tensor(0.0026, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# try form f(x) + g(x) * J2 + h(x) * J2^2 where f, g, h are functions of up to amp^3\n",
    "\n",
    "torch.manual_seed(1)\n",
    "f = nn.Sequential(nn.Linear(3 * N // 2, 1))\n",
    "g = nn.Sequential(nn.Linear(3 * N // 2, 1))\n",
    "h = nn.Sequential(nn.Linear(3 * N // 2, 1))\n",
    "\n",
    "opt = torch.optim.LBFGS(itertools.chain(f.parameters(), g.parameters(), h.parameters()), lr=0.05)\n",
    "floss = torch.nn.BCELoss()\n",
    "epochs = 200; datarate = epochs // 5\n",
    "for i in range(epochs):\n",
    "    def closure():\n",
    "        opt.zero_grad()\n",
    "        pred = torch.sigmoid(f(cub_amp_j2) + g(cub_amp_j2) * j2_data + h(cub_amp_j2) * j2_data ** 2)\n",
    "        loss = floss(pred, j2_signs)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    loss = opt.step(closure)\n",
    "    if i % datarate == 0:\n",
    "        print(loss.item())\n",
    "print(loss.item())\n",
    "print(torch.nn.MSELoss()(torch.round(torch.sigmoid(f(cub_amp_j2) + g(cub_amp_j2) * j2_data + h(cub_amp_j2) * j2_data ** 2)), j2_signs))\n",
    "# seed 0 gets to 0.0087 at some point but then goes to 12?\n",
    "# seed 1 gets to 0.0076, 0.0026 after rounding --> 240 mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4afb2155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002597402548417449\n"
     ]
    }
   ],
   "source": [
    "print(torch.nn.MSELoss()(torch.round(torch.sigmoid(f(cub_amp_j2) + g(cub_amp_j2) * j2_data + h(cub_amp_j2) * j2_data ** 2)), j2_signs).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "db6264bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239.9999954737723"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.002597402548417449 * 92400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e13e962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.77404499053955\n",
      "0.06770095974206924\n",
      "0.01351476926356554\n",
      "4.1072066636616e-08\n",
      "2.6715666123777737e-08\n",
      "2.0031466263503717e-08\n",
      "rounded loss\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# try form f(x) + g(x) * J2 + h(x) * J2^2 + p(x) * J2^3 where f, g, h, p are poly of up to amp^3\n",
    "\n",
    "torch.manual_seed(0)\n",
    "f = nn.Sequential(nn.Linear(3 * N // 2, 1))\n",
    "g = nn.Sequential(nn.Linear(3 * N // 2, 1))\n",
    "h = nn.Sequential(nn.Linear(3 * N // 2, 1))\n",
    "p = nn.Sequential(nn.Linear(3 * N // 2, 1))\n",
    "opt = torch.optim.LBFGS(itertools.chain(f.parameters(), g.parameters(), h.parameters(), p.parameters()), lr=0.05)\n",
    "floss = torch.nn.BCELoss()\n",
    "epochs = 200; datarate = epochs // 5\n",
    "for i in range(epochs):\n",
    "    def closure():\n",
    "        opt.zero_grad()\n",
    "        pred = torch.sigmoid(f(cub_amp_j2) + g(cub_amp_j2) * j2_data + h(cub_amp_j2) * j2_data ** 2 + p(cub_amp_j2 * j2_data ** 3))\n",
    "        loss = floss(pred, j2_signs)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    loss = opt.step(closure)\n",
    "    if i % datarate == 0:\n",
    "        print(loss.item())\n",
    "print(loss.item())\n",
    "print('rounded loss')\n",
    "print(torch.nn.MSELoss()(torch.round(torch.sigmoid(f(cub_amp_j2) + g(cub_amp_j2) * j2_data + h(cub_amp_j2) * j2_data ** 2 + p(cub_amp_j2 * j2_data ** 3))), j2_signs).item())\n",
    "# seed 0 makes no mistakes\n",
    "# --> 76 parameter summarization of 92400 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c2dbb2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 8.1331e+04,  2.2957e+03, -6.2607e+03, -1.1200e+04, -5.0462e+04,\n",
      "          9.2759e+02, -8.6915e+03, -1.2002e+02, -1.6636e+03,  4.5797e+02,\n",
      "          1.1491e+04,  2.5280e+03, -8.1008e+01, -1.4897e+03, -6.5714e+01,\n",
      "         -9.3733e+02, -1.4635e+03, -1.7452e+03]], requires_grad=True), Parameter containing:\n",
      "tensor([35.6799], requires_grad=True)]\n",
      "[Parameter containing:\n",
      "tensor([[ 34290.3125,  -7816.3193,   -679.1583,   -123.8333, -18169.7969,\n",
      "            303.3008, -17494.8066,  -4301.6851,   4433.3296,  11624.6035,\n",
      "          16429.1602,   1185.9393,   2524.9736,   8240.8779,    -75.0549,\n",
      "           3715.2371,   -685.3488,   4520.1831]], requires_grad=True), Parameter containing:\n",
      "tensor([157.4534], requires_grad=True)]\n",
      "[Parameter containing:\n",
      "tensor([[ 13510.0361,  -6888.7915,    292.3169,  -3520.8335,  -7518.8770,\n",
      "           -527.6835, -16550.3047,  -4387.2417,   3514.6880,  -3768.8623,\n",
      "           9109.3232,  -2145.2217,  -2453.3430, -13142.0615,  -1477.1410,\n",
      "         -12510.4746,  -5997.4502,  -8852.3330]], requires_grad=True), Parameter containing:\n",
      "tensor([-182.8735], requires_grad=True)]\n",
      "[Parameter containing:\n",
      "tensor([[ 6412.8047, -2721.5266,   552.8902, -2853.2974, -2603.3721,   338.5330,\n",
      "         -6844.2554,  3239.3486,  2798.7808, -2597.3491,  6835.7441,  1333.4746,\n",
      "          6510.7930,  8990.1748,   668.1271, 11719.4053,  4639.2549,  5167.6069]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([36.0488], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "for part in [f, g, h, p]:\n",
    "    print([n for n in part.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cf940d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
